##1.3. Kernel ridge regression
内核岭回归（KRR）结合岭回归（含L2-范数正则化的线性最小二乘法）与核技巧。它会在由数据和其内核产生的核空间内学习一个线性函数。对于非线性核，它在原始空间里对应非线性函数。

`KernelRidge`学习模型的形式与支持向量回归是相同的。然而，使用的损失函数不同：KRR使用平方误差损失而SVR使用![](http://scikit-learn.org/stable/_images/math/19bc0073dde1bcd1a8e6a32b251e80cced668f04.png)不敏感的损失，两者都与L2正则化结合。与SVR相反，`KernelRidge`可以在封闭形式下拟合并且中等数据规模要更快。另一方面，由于`KernelRidge`学习的模型是非稀疏的，而`SVR`当![](http://scikit-learn.org/stable/_images/math/defc8dedc4e1c71aa65da56c385f2d8681f2ed4d.png)会学习一个稀疏的模型，所以预测时`KernelRidge`速度要慢。

下图比较了
`KernelRidge`和`SVR`在人工数据集上的表现，它由一个正弦目标函数通过强行添加加噪声在每个第五个数据上。`KernelRidge`和`SVR`学习的模型被绘制出来，其中复杂度/正则化和RBF核的带宽已经通过网格搜索最优化过了。学习的函数是相似的；然而，`KernelRidge`学习花费的时间大概是`SVR`的7倍（同时使用了网格搜索）。然而，对于100000个目标值的预测时间是SVR的3倍，因为`SVR`仅用了大约1/3的数据点作为支持向量去学习了一个稀疏模型。

<center>![](http://scikit-learn.org/stable/_images/plot_kernel_ridge_regression_0011.png)</center>

另一幅图比较了`KernelRidge`和`SVR`对于不同大小的训练集上的拟合和预测时间。中等数据规模（样本数<1000)拟合`KernelRidge`要比`SVR`要快；然而对于大规模的训练集`SVR`表现要更好。对于预测时间，`SVR`在所有大小的数据集上都要比`KernelRidge`好，因它学习了一个稀疏解。记住稀疏度和预测时间依赖于`SVR`的![](http://scikit-learn.org/stable/_images/math/19bc0073dde1bcd1a8e6a32b251e80cced668f04.png)和![](http://scikit-learn.org/stable/_images/math/2bcc65482aa8e15cd4c9e9f2542451fb4e971a91.png)参数；![](http://scikit-learn.org/stable/_images/math/51c3f634737f3900a622e86675e34390c43102a7.png)会对应于密集模型。


<center>![](http://scikit-learn.org/stable/_images/plot_kernel_ridge_regression_0021.png)</center>
