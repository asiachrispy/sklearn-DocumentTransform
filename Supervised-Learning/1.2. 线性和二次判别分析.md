## 1.2. 线性和二次判别分析
线性判别分析（`discriminant_analysis.LinearDiscriminantAnalysis`）和二次判别分析（`discriminant_analysis.QuadraticDiscriminantAnalysis`）是两种分类方法，与名字一样，分别为线性和二次决策面。


这些分类器是有吸引力的，因为他们有封闭形式的解决方案，可以很容易地计算，本质上是多类，已被证明在实践中很好地工作，而且没有超参数调整。

<center><img alt="ldaqda" src="http://scikit-learn.org/stable/_images/plot_lda_qda_0011.png" style="width: 640.0px; height: 480.0px;"></center>


该图显示了线性判别分析和二次判别分析的决策边界。底行表明，线性判别分析只能学线性边界，而二次判别分析可以学习二次边界，因此更加灵活。

### 1.2.1. 利用线性判别分析来降维
`discriminant_analysis.LinearDiscriminantAnalysis`可以用来进行监督降维，通过将输入数据投影到一个可以最大化类间差异的线性子空间（下面的数学部分会详细讨论）。输出的维数必须小于类别数，所以这是一个一般的相当强烈的降维，只在一个多类设置中合理。

这个由`discriminant_analysis.LinearDiscriminantAnalysis.transform`实现。所需的维数可以使用`n_components`构造参数来设置。此参数对`discriminant_analysis.LinearDiscriminantAnalysis.fit`或`discriminant_analysis.LinearDiscriminantAnalysis.predict`没有影响。

### 1.2.2. LDA和QDA分类器的数学表达
LDA和QDA都可以从简单的概率模型（模型化数据的类条件分布![](http://scikit-learn.org/stable/_images/math/6349652f4f3fed24cf0e66ba44b19b5c170cae96.png)）推导出。预测可以通过贝叶斯规则获得：
<center>![](http://scikit-learn.org/stable/_images/math/9e7b9be3116147caceb05da29eb4a66905f66481.png)</center>
我们通过最大化条件概率选择类k。

进一步来说，对于线性和二次判别分析，![](http://scikit-learn.org/stable/_images/math/e69ebd95e385ac5c254ef15635c37b01668decbf.png)通过多元高斯分布密度来模型化：
<center>
![](http://scikit-learn.org/stable/_images/math/048bd26b8085a30c1baddcc048621fff532ffe47.png)</center>

为了使用这个模型作为一个分类器，我们只需要从训练数据的类先验概率![](http://scikit-learn.org/stable/_images/math/ca8cad6134f86bef8de0efaa68c0d24323256728.png)（通过类k实例的比例），类均值![](http://scikit-learn.org/stable/_images/math/e0f4c204f26e9afa4f5bbeda60af5262e624c148.png)（通过经验样本类均值）和协方差矩阵（无论是经验类样本协方差矩阵，或由正规化估计：见下面收缩部分）来估计。

对于LDA，每个类的高斯分布共享同样的协方差矩阵![](http://scikit-learn.org/stable/_images/math/5a0d5681512c9237a828e2667fdf0499af7fad60.png)。这导致线性决策表面间，可以通过对数概率比![](http://scikit-learn.org/stable/_images/math/e97ecabcc51db3bd271f1b5b0cf3fcebdd989108.png)可见。

<center>
![](http://scikit-learn.org/stable/_images/math/9dd706ccca2f0ca465f83a2ee3dc72e7ea904c7c.png)</center>

对于QDA，对于高斯分布的协方差矩阵![](http://scikit-learn.org/stable/_images/math/07b943c6dd1061bde470bebfa3e9b5e373216870.png)没有任何假设，所以导致了二次决策表面。

	Note： 和高斯贝叶斯的关系
	
	如果QDA模型假定协方差矩阵是对角矩阵，那么这意味着我们假定类是条件独立的，并且所得分类器与高斯朴素贝叶斯分类器`naive_bayes.GaussianNB`等价。
### 1.2.3. LDA降维的数学表达
为了理解LDA在降维上的使用，开始于上面解释的LDA分类规则的几何改写是有用的。
我们写K代表目标类的总数。在LDA中，由于我们假设所有类具有相同的协方差估计![](http://scikit-learn.org/stable/_images/math/19ac15bf260b22dcb61a1042c60259e4b0bfbd64.png)，我们可以重新调整数据让此协方差变成单位矩阵。
<center>
![](http://scikit-learn.org/stable/_images/math/e6da702a9f1619c67e81ec8cd9d976702ffa4a2e.png)</center>

数据缩放后再进行分类相当于发现最佳的类均值![](http://scikit-learn.org/stable/_images/math/5ee369a0219cf8053e73a6926064947a02d97434.png)（欧几里得空间与数据点最近）。但是这可以与将数据在所有类的所有![](http://scikit-learn.org/stable/_images/math/5ee369a0219cf8053e73a6926064947a02d97434.png)张成的K-1仿射子空间![](http://scikit-learn.org/stable/_images/math/dfe93a68f5d21483dc469d7d85fd6b43ce87f30a.png)上投影后一样好。这表明，LDA分类器隐含，这里有一个通过线性投影到K-1维的子空间的降维。

我们通过投影到一个线性子空间![](http://scikit-learn.org/stable/_images/math/dfe93a68f5d21483dc469d7d85fd6b43ce87f30a.png)（在投影后最大化![](http://scikit-learn.org/stable/_images/math/5ee369a0219cf8053e73a6926064947a02d97434.png)的方差，事实上，我们对转换的类均值[](http://scikit-learn.org/stable/_images/math/5ee369a0219cf8053e73a6926064947a02d97434.png)做了一个PCA形式的降维）来降维到一个选定的![](http://scikit-learn.org/stable/_images/math/0a5711c7a37994043b2bc3bb374adca232491762.png)。![](http://scikit-learn.org/stable/_images/math/0a5711c7a37994043b2bc3bb374adca232491762.png)与`discriminant_analysis.LinearDiscriminantAnalysis.transform`中的`n_components `参数相关联。

### 1.2.4. Shrinkage
Shrinkage是当训练样本数量相比于特征数目小时提高对协方差矩阵的估计的工具。在这种情况下，实证样本协方差是一个贫穷的估计。LDA Shrinkage可以通过设置`discriminant_analysis.LinearDiscriminantAnalysis `的`shrinkage`参数为`auto`来使用。这会自动确定最优的shrinkage参数。记住，现在的shrinkage仅当设置求解器为`lsqr`或`eigen`工作。

`shrinkage `参数也可以被手动设定在0和1之间。具体地，值0对应于无收缩率（即经验协方差矩阵将被使用）和1的值对应于完成收缩率（这意味着方差的对角矩阵将被用作用于协方差矩阵的估计）。
这个参数设置为这两个极值之间的值将估计一个缩水版的协方差矩阵。

<center>![](http://scikit-learn.org/stable/_images/plot_lda_0011.png)</center>

### 1.2.5. 估计算法
默认求解器是`svd`。它分类和变换都可以执行，并且它不依赖于协方差矩阵的计算。当特征数目大的时候这是一个有点。然而，`svd`求解器不能与收缩一起使用。
`lsqr`求解器是一种高效的算法，它仅适用于分类。它支持的收缩。

The ‘eigen’ solver is based on the optimization of the between class scatter to within class scatter ratio.它可以用于分类和变换，支持shrinkage。然而，`eigen`求解器需要计算协方差矩阵，所以它当特征数目很大时也许不适当。