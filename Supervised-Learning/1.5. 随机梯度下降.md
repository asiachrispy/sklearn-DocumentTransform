## 1.5. 随机梯度下降
随机梯度下降，对于损失收敛的线性分类器（例如`线性SVM`和 `Logistic Regression`），是一种简单却高效的学习算法。尽管SGD围绕在机器学习领域很长一段时间，但是只是最近在大规模学习范畴内受到大量的关注。

在经常遇到的文本分类和自然语言处理，SGD已经成功的应用到大规模和稀疏学习问题。给定的数据是稀疏的，这个模块的分类器很容易扩展到超过10^5训练实例和超过10^5的特征问题。

随机梯度下降的优点：


- 高效

- 易于实现（有很多调整代码的机会）

随机梯度下降的缺点：

- SGD需要许多的超参数，如正则化参数和迭代次数

- SGD对特征缩放敏感

### 1.5.1. 分类
	Warning：在训练模型之前确保你已经打乱了你的训练数据，或者使用shuffle=True来在每一次迭代后打乱数据。

`SGDClassifier `类为分类实现了支持不同损失函数和惩罚的随机梯度下降学习规则。

<center><img alt="../_images/plot_sgd_separating_hyperplane_0011.png" src="http://scikit-learn.org/stable/_images/plot_sgd_separating_hyperplane_0011.png" style="width: 600.0px; height: 450.0px;"></center>

和其他分类器一样，SGD `fit` 接受两个数组：训练样本X（`[n_samples, n_features]` ），训练样本的目标值（类标签）（`[n_samples]`）。
<pre><code>>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0., 0.], [1., 1.]]
>>> y = [0, 1]
>>> clf = SGDClassifier(loss="hinge", penalty="l2")
>>> clf.fit(X, y)
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)</code></pre>

在拟合之后，模型可以使用`predict`预测新值：
<pre><code>>>> clf.predict([[2., 2.]])
array([1])</code></pre>

SGD拟合一个线性模型，成员`coef_`持有模型参数：
<pre><code>>>> clf.coef_                                         
array([[ 9.9...,  9.9...]])</code></pre>

成员`intercept_`持有截距（又名偏移或偏置）：
<pre><code>>>> clf.intercept_                                    
array([-9.9...])</code></pre>

是否模型应该使用偏移，例如一个基础的超平面，使用`fit_intercept`来控制。

使用` SGDClassifier.decision_function`得到距离超平面的签名距离：
<pre><code>>>> clf.decision_function([[2., 2.]])                 
array([ 29.6...])</code></pre>

具体的损失函数可以设置`loss`参数。`SGDClassifier`支持下面的损失函数：

- `loss="hinge"`：（软间隔）线性支持向量机

- `loss="modified_huber"`：平滑hinge 损失

- `loss="log"`：logistic regression

- 和下面的所有回归损失

前两个损失函数是懒惰的，他们仅在一个样本违反间隔约束时更新参数，这使得训练十分高效并且即使使用L2惩罚也可能得到稀疏模型。

使用 `loss="log"` or `loss="modified_huber"`可以使用`predict_proba `方法，它为每一个样本给出了概率估计![](http://scikit-learn.org/stable/_images/math/25a9728b809a691ce3374eaeab6255410cbff5d9.png)向量：
<pre><code>>>> clf = SGDClassifier(loss="log").fit(X, y)
>>> clf.predict_proba([[1., 1.]])                      
array([[ 0.00...,  0.99...]])</code></pre>

具体的惩罚可以设置`penalty `参数。SGD支持下面的惩罚：

- `penalty="l2"`: L2 norm penalty on `coef_`.
- `penalty="l1"`: L1 norm penalty on `coef_`.
- `penalty="elasticnet"`: Convex combination of L2 and L1;<br>` (1 - l1_ratio) * L2 + l1_ratio * L1`

默认设置是`penalty="l2"`。L1惩罚降导致稀疏解，使得大多数系数为0.弹性网解决了L1惩罚在存在高度相关的属性时的不足。参数`l1_ratio `控制了L1和L2惩罚的凸结合。

`SGDClassifier`支持多类分类，以“one vs all”策略结合多个二元分类器。对于K类分类问题学习K个分类器，每个分类器通过对其中一例（+）和剩余所有类（-）进行学习。在测试期间，我们计算每个分类器的置信度，然后选择最高置信度的类。下面图是对`iris`数据集进行OVA方法的分类。虚线代表每个OVA分类器，背景颜色展示了三个分类器结合的决策面。
<center><img alt="../_images/plot_sgd_iris_0011.png" src="http://scikit-learn.org/stable/_images/plot_sgd_iris_0011.png" style="width: 600.0px; height: 450.0px;"></center>

在这个多类分类中，`coef_`是一个二维数组（`shape=[n_classes, n_features] `），`intercept_`是一个一维数组（`shape=[n_classes]`）。`coef_`的每一行都代表了一个OVA分类器的权重向量，类按升序索引（参见`classes_`）。记住，因为他们要求创建概率模型，所以`loss="log"` 和 `loss="modified_huber"`更适合OVA分类。

`SGDClassifier` 支持类权重和样本权重，通过设置`class_weight`和`sample_weight`。

`SGDClassifier`支持平均SGD（ASGD）。平均可以通过设置`average=True`。ASGD工作原理是在每一个样本每一次迭代之后平均SGD平面的系数。当使用ASGD时，学习率可以更大些甚至不断带领一些数据集在训练时间加速。

对于`logistic loss`分类器，另一个平均策略SGD的变种可以通过随机平均梯度下降（SAG）获得，在`LogisticRegression`作为一个求解器可以获得。

### 1.5.2. 回归
`SGDRegressor`类为拟合线性回归模型提供了不同的损失函数和惩罚的随机梯度下降学习算法。`SGDRegressor`非常适合大量训练样本（> 10 000）的回归问题，对于其他问题我们推荐`Ridge`, `Lasso`, or `ElasticNet`。

具体的损失函数可以设置loss参数。`SGDRegressor`支持下面的损失函数：

- `loss="squared_loss"`: Ordinary least squares,
- `loss="huber"`: Huber loss for robust regression,
- `loss="epsilon_insensitive"`: linear Support Vector Regression.

`Huber` 和`epsilon-insensitive`损失可以用作鲁棒回归。不敏感区的宽度可以通过参数`epsilon`指定。这个参数依赖于目标变量的规模。

`SGDRegressor` 和`SGDClassifier`一样支持平均SGD。平均可以通过设置`average=True`。

对于二乘损失和L2惩罚回归，另一个平均策略SGD的变种可以通过随机平均梯度下降（SAG）获得，在 `Ridge`作为一个求解器可以获得。

### 1.5.3. 随机梯度下降和稀疏数据
	Note：稀疏实现相较于密集实现会产生一个轻微不同的结果，这是由于对于截距的收敛学习率。
 内建以`scipy.sparse`格式对给定的稀疏数据矩阵提供支持。为了效率最大化，使用CSR矩阵（`scipy.sparse.csr_matrix`）。

### 1.5.4. 复杂度
SGD的主要优点是它的高效性，与训练样本数基本线性相关。如果X是一个形状(n,p)的矩阵，代价为![](http://scikit-learn.org/stable/_images/math/93aa7f61c5ca120b643faa21a01cf5c504851f8c.png)，其中k是迭代次数，![](http://scikit-learn.org/stable/_images/math/2c5d2ee447e5be49d4e3b1e7d40c49c76329e686.png) 是每一个样本的非零属性平均数目。

但是最近的一些理论成果表明为了得到目标最优精度的运行时间不会随着训练集规模而增大。

### 1.5.5. 实际使用小贴士
- 随机梯度下降对特征规模敏感，所以强烈建议规范你的数据。例如，规范每一个输入属性到 [0,1] or [-1,+1],或标准化它（均值=0，方差=1）。记住，同样的规范化也必须应用到测试向量上。这可以很容易地使用`StandardScaler`做到：
	<pre><code>from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)  # Don't cheat - fit only on training data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)  # apply same transformation to test data</code></pre>

	如果你的属性具有的特性尺度（例如单词频率或指示器功能）不需要缩放。

- 发现合理的正则化项![](http://scikit-learn.org/stable/_images/math/ad59b6e24a4a00ac621801f8d7513d68be654ab5.png)最好使用`GridSearchCV`，通常在范围` 10.0**-np.arange(1,7)`内。

- 经验表示，我们发现SGD在大约10^6个训练样本之后会收敛。因此，一个对迭代次数的第一合理猜测是`n_iter = np.ceil(10**6 / n)`，`n`是训练集大小。

- 如果你将SGD应用到使用PCA进行特征提取，我们发现通过一些常数C来规范化特征值好让训练训练数据的平均L2范数等于1。

- 我们发现平均SGD在大量特征和一个高的`eta0`下工作最好。

### 1.5.6. 数学公式
对于跟定的训练样本![](http://scikit-learn.org/stable/_images/math/4af6492b1bf93d4be0e1f0c75f85f65170ed94c0.png) ![](http://scikit-learn.org/stable/_images/math/ed7238746ec0fb27be38187ff0576c83ecc0df8f.png)和![](http://scikit-learn.org/stable/_images/math/bb280b0cec96f69df3ad1c8befd46a0a95e843ac.png)，我们的目标是要学习一个线性模型![](http://scikit-learn.org/stable/_images/math/3bd8e83af63059e1332f577c656fdff3e8158e74.png) 模型参数![](http://scikit-learn.org/stable/_images/math/e0f293430b347291b63495a02f47d30e35fa7229.png) 模型截距![](http://scikit-learn.org/stable/_images/math/8b75b89abc229b96382c47277d6f475f634940d9.png)。为了做预测，我们简单的看一下![](http://scikit-learn.org/stable/_images/math/14546c27a7b929642f7840acca5f851c503ea109.png)的符号。一个通常的选择是发现最小化下列正则化训练误差的参数：
<center>![](http://scikit-learn.org/stable/_images/math/4782697c94f74995ee99624f00633bba53f5245f.png)</center>

![](http://scikit-learn.org/stable/_images/math/0a5711c7a37994043b2bc3bb374adca232491762.png)是损失函数，![](http://scikit-learn.org/stable/_images/math/9d86170e7de539c0ff999de09621ee0c7b6c8ed0.png)是正则化项（又名惩罚）惩罚模型复杂度；![](http://scikit-learn.org/stable/_images/math/7f2c98bf462cba6083cf18483ba9510e3c2fd3d3.png)是一个非负超参数。

对于不同分类器的不同的![](http://scikit-learn.org/stable/_images/math/0a5711c7a37994043b2bc3bb374adca232491762.png)选择如下：

- `Hinge`: (soft-margin) Support Vector Machines.
- `Log`: Logistic Regression.
- `Least-Squares`: Ridge Regression.
- `Epsilon-Insensitive`: (soft-margin) Support Vector Regression.

所有上述损失函数都可被认为是0/1损失的上界：
<center><img alt="../_images/plot_sgd_loss_functions_0011.png" src="http://scikit-learn.org/stable/_images/plot_sgd_loss_functions_0011.png" style="width: 600.0px; height: 450.0px;"></center>

常用的正则化项![](http://scikit-learn.org/stable/_images/math/9d86170e7de539c0ff999de09621ee0c7b6c8ed0.png)包括：
<div><ul class="simple">
<li>L2 norm: <img class="math" src="http://scikit-learn.org/stable/_images/math/a16847a5124181d9b7752831345def32c7688718.png" alt="R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2">,</li>
<li>L1 norm: <img class="math" src="http://scikit-learn.org/stable/_images/math/9dcd00d73a520a96fcccfa317471aab11d8b1ce7.png" alt="R(w) := \sum_{i=1}^{n} |w_i|">, which leads to sparse
solutions.</li>
<li>Elastic Net: <img class="math" src="http://scikit-learn.org/stable/_images/math/6d786bc0d556134fd2cd2ff743dd4b1a711bb541.png" alt="R(w) := \frac{\rho}{2} \sum_{i=1}^{n} w_i^2 + (1-\rho) \sum_{i=1}^{n} |w_i|">, a convex combination of L2 and L1, where <img class="math" src="http://scikit-learn.org/stable/_images/math/f574498915fa9e02eeb5141c24835d077eba3e75.png" alt="\rho"> is given by <code class="docutils literal"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">l1_ratio</span></code>.</li>
</ul>
</div>

下图展示了当![](http://scikit-learn.org/stable/_images/math/c12181d671649ed5892696252202bba516d28d3a.png)不同正则化项在参数空间的轮廓。
<center><img alt="../_images/plot_sgd_penalties_0011.png" src="http://scikit-learn.org/stable/_images/plot_sgd_penalties_0011.png" style="width: 600.0px; height: 450.0px;"></center>

#### 1.5.6.1. SGD
随机梯度下降是无约束问题的优化方法。与批梯度下降相反，SGD一次只考虑单个训练样本的梯度并以此近似真正的梯度![](http://scikit-learn.org/stable/_images/math/d5d48823181b7f7a9e96c9cbd3bb70d04abdbd9d.png)。

该类SGDClassifier实现一阶SGD学习程序。该算法为在训练样本上迭代并根据给定的更新规则为每个训练样本更新模型参数。
<center>![](http://scikit-learn.org/stable/_images/math/98cdc3aed40cb93594dbaaf045ea3e1abbd8edcb.png)</center>

![](http://scikit-learn.org/stable/_images/math/9172efc523d7488d4c3ed299d0be813de01503b8.png)是学习率控制着在参数空间内的步伐。截距![](http://scikit-learn.org/stable/_images/math/5e87bf41a96deddf6cb485ff530f153f2590e9cc.png)被近似地更新但是没有正则化项。


学习率![](http://scikit-learn.org/stable/_images/math/9172efc523d7488d4c3ed299d0be813de01503b8.png) 可以是常量或逐渐衰减。对于分类默认的学习率策略（`learning_rate='optimal'`）给定如下：

<center>![](http://scikit-learn.org/stable/_images/math/f12f7e91b0d9d96b5e417194a1a348c718b981b1.png)</center>

其中![](http://scikit-learn.org/stable/_images/math/ef9270877405055756d345facd044e4ab297f858.png)是时间步长（`n_samples * n_iter time steps`），![](http://scikit-learn.org/stable/_images/math/fb761c2e199ad45ccf14767f48b88169479d840f.png)由基于 Léon Bottou 提出的启发式决定，这样，预期的初始更新与权重的预期大小可比（假设训练样本的范数近似为1）。精确定义可以在`BaseSGD`的`_init_t`中找到。

对于回归问题默认的学习率策略是逆伸缩（learning_rate='invscaling'），给定如下：
<center>![](http://scikit-learn.org/stable/_images/math/1e8904a391eec39276c9794f76d1b3a247e5fdf0.png)</center>

其中![](http://scikit-learn.org/stable/_images/math/17ed5419fbfe317f160a2be3da9c6f0ce53b5b86.png)和![](http://scikit-learn.org/stable/_images/math/138a8dd99b6da12de6f02b76afeda6e5885a7f7c.png)是超参数，用户通过`eta0 `和`power_t`指定。

对于一个常数学习率使用` learning_rate='constant'`和`eta0`来指定学习率。

模型参数可以通过成员`coef_ `和`intercept_`获得：

- Member `coef_` holds the weights ![](http://scikit-learn.org/stable/_images/math/8659700e6646cd91bc02c32affaa5ec046ee9935.png)
- Member `intercept_` holds ![](http://scikit-learn.org/stable/_images/math/5e87bf41a96deddf6cb485ff530f153f2590e9cc.png)

### 1.5.7. 实现细节






