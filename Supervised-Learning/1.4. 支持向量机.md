## 1.4. 支持向量机
支持向量机是用来分类、回归和离群点探测的一揽子监督学习方法。

支持向量机的优点有：<br>
　　1. 高维有效<br>
　　2. 当特征数大于样本数仍然有效<br>
　　3. 决策函数使用训练点的一个子集<br>
　　4. 多样性：可以为决策函数指定不同的核函数，常用核函数被提供了，但是也可以自定义核函数。<br>


支持向量机的缺点包括：<br>
　　1. 如果特征数远远大于样本数，该方法很可能表现不好<br>
　　2. SVMs无法直接提供估计的可能性，这些使用昂贵的5折交叉验证计算得来。<br>


` scikit-learn`中的支持向量机同时支持密度和稀疏样本向量作为输入。然而，然而，使用SVM为稀疏数据进行预测，那么它必须已经在稀疏数据上拟合。为了获得最佳性能，使用C-ordered `numpy.ndarray` (dense) or `scipy.sparse.csr_matrix` (sparse) with `dtype=float64`。

### 1.4.1. 分类
`SVC`, `NuSVC` 和 `LinearSVC` 是能够对数据执行多类分类的类。

<center>![](http://scikit-learn.org/stable/_images/plot_iris_0012.png)</center>

`SVC`和 `NuSVC`是相似的方法，但是接受的参数集合有一点轻微的不同并且拥有不同的数学表达。另一方面，`LinearSVC`是支持向量分类为`linear`核的另一种实现。记住，`LinearSVC`不接受关键字`kernel`，因为它假定是`linear`。它也缺乏一些`SVC`和 `NuSVC`的成员，像`support_`。

像其他分类器一样，`SVC`, `NuSVC` 和 `LinearSVC`接受两个数组：一个大小`[n_samples, n_features]`的训练样本数组X，一个大小`[n_samples]`的类标签数组y。

<pre><code>>>> from sklearn import svm
>>> X = [[0, 0], [1, 1]]
>>> y = [0, 1]
>>> clf = svm.SVC()
>>> clf.fit(X, y)  
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
</code></pre>

拟合之后，模型可以用来预测新的值：
<pre><code>>>> clf.predict([[2., 2.]])
array([1])
</code></pre>

SVMs决策函数依赖于训练数据的一些子集，称为支持向量。这些支持向量的一些属性可以通过成员`support_vectors_`，`support_ `和` n_support`。
<pre><code>>>> # get support vectors
>>> clf.support_vectors_
array([[ 0.,  0.],
       [ 1.,  1.]])
>>> # get indices of support vectors
>>> clf.support_ 
array([0, 1]...)
>>> # get number of support vectors for each class
>>> clf.n_support_ 
array([1, 1]...)
</code></pre>

#### 1.4.1.1. 多类分类
对于多类分类`SVC`和`NuSVC`通过“一对一”方法实现。如果类别数是`n_class`，那么将有`n_class * (n_class - 1) / 2`个分类器被建造，每个训练两类数据。为了为其他分类器提供兼容的接口，`decision_function_shape`允许折叠“一对一”的结果分类器成一个形状`(n_samples, n_classes)`的决策函数。

<pre><code>>>> X = [[0], [1], [2], [3]]
>>> Y = [0, 1, 2, 3]
>>> clf = svm.SVC(decision_function_shape='ovo')
>>> clf.fit(X, Y) 
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
>>> dec = clf.decision_function([[1]])
>>> dec.shape[1] # 4 classes: 4*3/2 = 6
6
>>> clf.decision_function_shape = "ovr"
>>> dec = clf.decision_function([[1]])
>>> dec.shape[1] # 4 classes
4
</code></pre>

决策函数的完整描述参见数学表达。

注意，`LinearSVC`也实现了一个替代的多类策略，所谓的多类SVM由Crammer 和 Singer规定，通过使用可选的`multi_class='crammer_singer'`。这个方法是兼容的，是不正确的“一对多”分类器。事实上，“一对多”分类器通常是首选的，因为结果总是相似的，但是运行时间要显著少。

对于“一对多”`LinearSVC `属性`coef_ `和` intercept_`形状分别是`[n_class, n_features] `和` [n_class] `。系数和截距的每一行都与`n_class`个“一对多”分类器中的一个相对应，按”one“类进行排序。

“一对一”的`SVC`，属性的布局有一点复杂。对于线性核函数，`coef_`和`intercept_`的布局与上述的`LinearSVC`相似，除了`coef_`的形状是[n_class * (n_class - 1) / 2, n_features]，相关的是许多的二元分类器。类0到n的排序是：“0 vs 1”, “0 vs 2” , ... “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . . . “n-1 vs n”。

`dual_coef_ `的形状是 `[n_class-1, n_SV]`，把握布局有点难。 列与支持向量参与的任意的`n_class * (n_class - 1) / 2`个“one-vs-one”分类器中。每个支持向量在`n_class - 1`个分类器中使用。每行的`n_class - 1`项都与这些分类的双系数相关联。

通过一个例子也许更好理解：<br>


考虑一个3类问题，类0有3个支持向量![](http://scikit-learn.org/stable/_images/math/4b88f1d1d8196c7bbe4ae71a937f701fee59589f.png)，类1和类2分别有两个支持向量![](http://scikit-learn.org/stable/_images/math/749ba150f0874a6aeb9951c4d3874994bb8ce7a8.png)和![](http://scikit-learn.org/stable/_images/math/55717c27d8dc9fc74a47ab9c34e12fdee568d9ba.png)。对于每个支持向量![](http://scikit-learn.org/stable/_images/math/ae16e9774b173bfa6489c04d4d7b581ff31a55a5.png)都有两个双系数。让我们把支持向量![](http://scikit-learn.org/stable/_images/math/ae16e9774b173bfa6489c04d4d7b581ff31a55a5.png)的在类i和k间的系数叫做![](http://scikit-learn.org/stable/_images/math/e26798dbf37644239db0d4660e7d514993e0c293.png)。那么`dual_coef_`看起来像这样：
<table border="1" class="docutils">
<colgroup>
<col width="36%">
<col width="36%">
<col width="27%">
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><img class="math" src="http://scikit-learn.org/stable/_images/math/a1b925a4dbbe65450fbebb81572f6222a434b9a0.png" alt="\alpha^{0}_{0,1}"></td>
<td><img class="math" src="http://scikit-learn.org/stable/_images/math/2d68a3b7f2e7b7d869d407df04909c805063cca5.png" alt="\alpha^{0}_{0,2}"></td>
<td rowspan="3">Coefficients
for SVs of class 0</td>
</tr>
<tr class="row-even"><td><img class="math" src="http://scikit-learn.org/stable/_images/math/8c43465f3d8574113c7e15638bbf2fae35ee3896.png" alt="\alpha^{1}_{0,1}"></td>
<td><img class="math" src="http://scikit-learn.org/stable/_images/math/83a6b3500cafea6d2bc19e8dc4550cfc77f8b04a.png" alt="\alpha^{1}_{0,2}"></td>
</tr>
<tr class="row-odd"><td><img class="math" src="http://scikit-learn.org/stable/_images/math/f812188f63141acf67e13dca94938ea66c93d8b3.png" alt="\alpha^{2}_{0,1}"></td>
<td><img class="math" src="http://scikit-learn.org/stable/_images/math/132dd1872743cc6a09b7224a438f031de75f935d.png" alt="\alpha^{2}_{0,2}"></td>
</tr>
<tr class="row-even"><td><img class="math" src="http://scikit-learn.org/stable/_images/math/c071d5b7948b79b8bd2232aae2939399a487a24c.png" alt="\alpha^{0}_{1,0}"></td>
<td><img class="math" src="http://scikit-learn.org/stable/_images/math/8efaf51c35183f8aa1a1c4ccaefd5a49b9dbef8c.png" alt="\alpha^{0}_{1,2}"></td>
<td rowspan="2">Coefficients
for SVs of class 1</td>
</tr>
<tr class="row-odd"><td><img class="math" src="http://scikit-learn.org/stable/_images/math/f639c61c3322f48687bf08827b851de13d220c0d.png" alt="\alpha^{1}_{1,0}"></td>
<td><img class="math" src="http://scikit-learn.org/stable/_images/math/76fb561a7220221493e4887d6c19668c2c0009c2.png" alt="\alpha^{1}_{1,2}"></td>
</tr>
<tr class="row-even"><td><img class="math" src="http://scikit-learn.org/stable/_images/math/1ec89a125496001eea33264978ca18b2f28136cb.png" alt="\alpha^{0}_{2,0}"></td>
<td><img class="math" src="http://scikit-learn.org/stable/_images/math/715efcbd1f7148dd4d8e1b3a6a9ea03d12907d8b.png" alt="\alpha^{0}_{2,1}"></td>
<td rowspan="2">Coefficients
for SVs of class 2</td>
</tr>
<tr class="row-odd"><td><img class="math" src="http://scikit-learn.org/stable/_images/math/d637f33c37a5688da805b551083d16e7a674f7d5.png" alt="\alpha^{1}_{2,0}"></td>
<td><img class="math" src="http://scikit-learn.org/stable/_images/math/8ab6fd0263c1e67950fc82aacea0ec613e7d37ed.png" alt="\alpha^{1}_{2,1}"></td>
</tr>
</tbody>
</table>

#### 1.4.1.2. 得分和概率
`SVC`方法`decision_function `为每一个样本给出每一个类的得分（或者在二元情况下给出每个样本的单个分数）。当构造选项`probability`被设置为`True`，类别成员的概率估计（通过`predict_proba`方法）将被启用。在二元情况下，概率使用普拉特缩放校准：对SVM的分数进行逻辑回归，通过在训练数据集上添加额外的交叉验证拟合。在多类情况下，this is extended as per Wu et al. (2004).

不用说，对于大规模数据集使用普拉特缩放交叉验证是非常昂贵的操作。此外，概率估计也许与得分不一致，在这个意义上，所述的分数“`argmax`”未必是概率的`argmax`。（例如，在二元分类中，一个样本也许通过`predict `进行标签，因为`predict_proba`中的一个类的概率<1/2）。普特拉方法也有理论问题。如果需要置信度分数，但是不必要概率，那么建议设置`probability=False`并且使用`decision_function `代替`predict_proba`。

#### 1.4.1.3. 不平衡问题
在希望给予某些类或某些个别样本更多的重要性的问题中，可以使用`class_weight` 和`sample_weight`。

`SVC`在`fit`里实现了一个关键字`class_weight `它是一个这个形式的字典`{class_label : value}`，`value`是>0的浮点数，它设置类`class_label `的`C`参数为`C * value`。


<center><img alt="../_images/plot_separating_hyperplane_unbalanced_0011.png" src="http://scikit-learn.org/stable/_images/plot_separating_hyperplane_unbalanced_0011.png" style="width: 600.0px; height: 450.0px;"></center>

`SVC`, `NuSVC`, `SVR`, `NuSVR` and `OneClassSVM` 在`fit`里实现了关键字`sample_weight`用于独立样本的权重。和`class_weight`相似，这些为第i个样本设置参数`C`为` C * sample_weight[i]`。

<center>![](http://scikit-learn.org/stable/_images/plot_weighted_samples_0011.png)</center>

### 1.4.2. 回归
支持向量分类也可以继承用来解决回归问题。这个方法叫做支持向量回归。

支持向量分类产生的模型仅依赖于训练数据的一个子集，因为建造模型的代价函数并不关心大多数超出边缘的训练点。类似地，支持向量回归产生的模型仅依赖于训练数据的一个子集，因为建造模型的代价函数会忽略任何靠近模型预测的训练数据。

这里有三种不同的支持向量回归实现：`SVR`, `NuSVR` 和 `LinearSVR`。 `LinearSVR`产生一个比`SVR`更快的实现但是仅考虑线性核，而`NuSVR`实现与`SVR`和`LinearSVR`有轻微的不同。

与分类相同，`fit`方法接受参数向量X,y，不过y是浮点数而不是整数：

<pre><code>>>> from sklearn import svm
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> clf = svm.SVR()
>>> clf.fit(X, y) 
SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
>>> clf.predict([[1, 1]])
array([ 1.5])</code></pre>

### 1.4.3. 密度估计，新奇检测
`OneClassSVM`用于新奇检测，给定一个样本集合，它会检测出这个集合的软边界并以此来判断新的数据点是否属于这个集合。

在这种情况下，因为它是一类非监督学习，`fit`方法仅用输入X，因为这里没有类标签。

更多参见<a href="http://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection">Novelty and Outlier Detection</a>

<center><img alt="../_images/plot_oneclass_0011.png" src="http://scikit-learn.org/stable/_images/plot_oneclass_0011.png" style="width: 600.0px; height: 450.0px;"></center>

### 1.4.4. 复杂度
支持向量机是有力工具，但它们的计算和存储要求随着训练向量的数量迅速增加。一个SVM的核心是一个二次规划问题（QP），从训练数据的其余部分分离出支持向量。这个二次规划求解器使用基于`libsvm`的实现规模在![](http://scikit-learn.org/stable/_images/math/7325ea0226849021035c250bebb23461ccf847dc.png)和![](http://scikit-learn.org/stable/_images/math/a93196c3d7dc949166a2193a44426878754eaa31.png)之间，依赖于`libsvm`在实际使用中的缓存效率（依赖于数据集）。如果数据时非常稀疏的，应该使用样本向量中非零特征的平均数量来代替![](http://scikit-learn.org/stable/_images/math/77d50c3510468fe9b613e6302375706598dfb13d.png)。

同样记住，对于线性情况，基于` liblinear`的`LinearSVC `实现要比对应的基于`libsvm`的`SVC`要更加高效，并且几乎可以线性扩展到 数百万的样本或特征。

### 1.4.5. 实际使用小贴士

- 避免数据复制：对于 `SVC`, `SVR`, `NuSVC` 和`NuSVR`，如果传入算法的数据不是“C-ordered”和双精度，那么在调用底层的C实现之前会进行复制。你可以通过`flags`属性检查一个给定的`numpy`数组是否是“C-contiguous”。

	对于` LinearSVC`（和`LogisticRegression`）任何传入的`numpy`数组都将进行复制并转换成稀疏数组（双精度floats和int32非零成分的索引）。如果你想在拟合一个大规模线性分类器但是不复制一个双精度“C-contiguous”的密集`numpy`数组作为输入的话，建议使用`SGDClassifier`。目标函数可以配置成与`LinearSVC`几乎相同。

- 核缓存大小：对于 `SVC`, `SVR`, `nuSVC` 和`NuSVR`，核缓存的大小对大规模问题的运行时间有强烈的影响。如果你有足够的`RAM`可以使用，推荐设置`cache_size`大于默认的200MB，例如500MB或1000MB。

- 设置参数`C`：默认`C=1`并且它是一个合理的默认选择。如果你的观测有很多噪声，你应该降低它。相比预测它与正则化更相关。

- 支持向量机不是尺度不变的，因此强烈建议规范你的数据。例如，将输入的每一个属性规范到 [0,1] o或[-1,+1]，或者标准化它（均值为0方差为1）。记住测试向量也要进行相同的规范化。详细细节请参见` Preprocessing data`。

-  `NuSVC`/`OneClassSVM`/`NuSVR` 的参数`nu`近似训练误差的分数和支持向量。

- 在`SVC`中，如果分类数据是不平衡的（例如正例多负例少），设置` class_weight='balanced'`或者尝试不同的惩罚系数`C`。

- 当进行拟合时，`LinearSVC`的底层实现使用一个随机数产生器进行特征选择。因此对于相同的输入数据拥有轻微不同的结果是正常的。如果发生了，请尝试更小的`tol`参数。

- ` LinearSVC(loss='l2', penalty='l1', dual=False)`使用L1惩罚输出一个稀疏解，例如，仅有一个特征权重的子集是不同于零的，并且有助于决策函数。增加`C`输出一个更复杂的模型（更多的特征被选择）。`C`使用`l1_min_c`会产生一个“null”模型（所有权重都等于0）。

### 1.4.6. 核函数
核函数可以是下列的任意一个：

- linear： ![](http://scikit-learn.org/stable/_images/math/b8ce75ac275fbe7a9cc88f535e164ba2397d3a85.png)

- polynomial：![](http://scikit-learn.org/stable/_images/math/59ec4d406b308d297b0e2e908f767564cd4e64ec.png)。![](http://scikit-learn.org/stable/_images/math/425d86ba2f2979d75b7535c2bcf92c33ed6b285a.png)通过关键字 `degree`指定, ![](http://scikit-learn.org/stable/_images/math/2ede365ad144ab396916ec60458da03860803078.png)通过 `coef0`指定。

- rbf：![](http://scikit-learn.org/stable/_images/math/47ed3b4a33cd152093bab738cf2002fa69c54c21.png)。![](http://scikit-learn.org/stable/_images/math/0ebb67342b546ca42a1c634b1ef03c893c4cdedb.png)通过`gamma`指定，必须大于0。

- sigmoid：![](http://scikit-learn.org/stable/_images/math/d768e9b981b47e886f24febd01391eeb3c2c98b6.png)。![](http://scikit-learn.org/stable/_images/math/2ede365ad144ab396916ec60458da03860803078.png)通过`coef0`指定。

不同的核函数在初始化的时候通过关键字指定：
<pre><code>>>> linear_svc = svm.SVC(kernel='linear')
>>> linear_svc.kernel
'linear'
>>> rbf_svc = svm.SVC(kernel='rbf')
>>> rbf_svc.kernel
'rbf'</code></pre>

####1.4.6.1 自定义核
你可以通过Python函数来自定义你自己的核函数，或者通过预计算Gram矩阵。

自定义核的分类器表现与其他分类器一样，除了：

- 属性`support_vectors_`现在是空的，只有支持向量的索引被存储在`support_`。

- `fit()`的第一个参数的引用（并不是复制）被存储起来为了未来的引用。如果在`fit()`和`predict()`之间数组有变化，你将会有一个无法预料的结果。

##### 1.4.6.1.1.  使用Python函数作为核
你也可以通过在构造时传入一个函数给关键字`kernel`来使用你自己的核函数。

你的核函数必须接受两个数组，形状`(n_samples_1, n_features)`, `(n_samples_2, n_features)`，并且返回一个核矩阵形状`(n_samples_1, n_samples_2)`。

下面的代码定义了一个线性核并且使用它创建了一个分类器实例：
<pre><code>>>> import numpy as np
>>> from sklearn import svm
>>> def my_kernel(X, Y):
...     return np.dot(X, Y.T)
...
>>> clf = svm.SVC(kernel=my_kernel)</code></pre>

##### 1.4.6.1.3. Parameters of the RBF Kernel
当训练一个使用RBF核的SVM，两个参数必须被考虑：`C`和`gamma`。参数`C`和其他所有SVM核一样，是训练样本的误分类和决策面的简单性的交易。一个低的`C`值使得决策面平滑，而一个高的`C`值旨在将所有的训练样本都正确分类。`gamma`定义了一个训练的样本有多大的影响。大的`gamma`表示，更靠近的例子必须有影响。

`C`和`gamma`的正确选择对于SVM的表现是至关重要的。一个建议是使用在`C`和`gamma`空间进行网格搜索（`sklearn.grid_search.GridSearchCV`）来选择好的值。

### 1.4.7. 数学表达
支持向量机在高维或无限维空间里构造一个超平面来进行分类、回归或其他任务。直观地说，一个超平面到最近的无论是哪个类的训练数据点的距离（函数间隔）是最大的，这个超平面就是完美的，因为一般来说，最大边缘意味着更小的泛化误差。

<center><img alt="../_images/plot_separating_hyperplane_0011.png" src="http://scikit-learn.org/stable/_images/plot_separating_hyperplane_0011.png" style="width: 600.0px; height: 450.0px;"></center>

##### 1.4.7.1. SVC
对于给定的训练向量![](http://scikit-learn.org/stable/_images/math/2c1db07a28fbde7396015ec811dcaf9d577f6679.png)，i=1,...,n,在二类问题中，![](http://scikit-learn.org/stable/_images/math/c6d6323bc6376953afef6b5b8a6689a5440357c4.png)，SVC解决下面的问题：
<center>![](http://scikit-learn.org/stable/_images/math/396704acdf11cc18d2d02b32275c0ee42d76b95e.png)</center>

对偶
<center>![](http://scikit-learn.org/stable/_images/math/2133ba457cf081dbcc03536c61648e52417fc80c.png)</center>

![](http://scikit-learn.org/stable/_images/math/630e3a780577ea7921e81ee2ac5237dd1802ec8d.png)是全是1的向量，![](http://scikit-learn.org/stable/_images/math/cb131e0f1390675e9f3219554f22bec1e387f75f.png)是上界，![](http://scikit-learn.org/stable/_images/math/7b1816c51f7d31275cd3ad400208fb7b3ce136a0.png)是![](http://scikit-learn.org/stable/_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png) by ![](http://scikit-learn.org/stable/_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png)的正半定矩阵，![](http://scikit-learn.org/stable/_images/math/778bba042f0a485da8e3297541320a94246be194.png)，  ![](http://scikit-learn.org/stable/_images/math/e8add116d50419f711117e1ff314e230537fccc8.png)是核。这里通过函数![](http://scikit-learn.org/stable/_images/math/10e009bdb83f96c5f47c58b34d5d4b12ef268d5b.png) 训练向量被隐式映射到高维(也许是无限维）空间中。

决策函数是：
<center>![](http://scikit-learn.org/stable/_images/math/765b4bda0ec7e64d4baa34e4f0e8cc01ca151e1a.png)</center>



**注意**：SVM模型由libsvm和liblinear使用C作为正则化参数进行推导，大多数其他的预测器使用alpha。两者关系是 ![](http://scikit-learn.org/stable/_images/math/560f9754fb09aec60beabe312f4d9427baf0f452.png)

这个参数可以通过`dual_coef_ `（持有![](http://scikit-learn.org/stable/_images/math/a684a4a28d9d3b9c31da95fe0408c235c1e1961f.png)）获得。`support_vectors_`（持有支持向量），并且`intercept_ `持有独立项![](http://scikit-learn.org/stable/_images/math/f574498915fa9e02eeb5141c24835d077eba3e75.png)。

##### 1.4.7.2. NuSVC
我们加入了一个新的参数来控制支持向量的个数和训练误差。参数![](http://scikit-learn.org/stable/_images/math/daf0484d7bd97c43bc8c73b35c3335345d24b8b4.png)训练误差的上界和支持向量的下界。

![](http://scikit-learn.org/stable/_images/math/d67fb61cfffca2cc069e083d76cb6220b4ca14c8.png)-SVC公式化是![](http://scikit-learn.org/stable/_images/math/2bcc65482aa8e15cd4c9e9f2542451fb4e971a91.png)-SVC的再参数化，因此数学上是等价的。

##### 1.4.7.3. SVR
对于给定的训练向量![](http://scikit-learn.org/stable/_images/math/2c1db07a28fbde7396015ec811dcaf9d577f6679.png)，并且![](http://scikit-learn.org/stable/_images/math/5c83c0acadd3ac75ed5baf206310b9b1a2b03f3a.png)![](http://scikit-learn.org/stable/_images/math/f348c73e654151e416e3c525b2ff7803590754ce.png)-SVR主要解决下面问题：

<center>![](http://scikit-learn.org/stable/_images/math/a4564c53a065f88d31b097b1b15a2db01e72bfd5.png)</center>

对偶
<center>![](http://scikit-learn.org/stable/_images/math/08c0a64f82c8a1bcf792aa6ffc9fda4281353e77.png)</center>

![](http://scikit-learn.org/stable/_images/math/630e3a780577ea7921e81ee2ac5237dd1802ec8d.png)是全是1的向量，![](http://scikit-learn.org/stable/_images/math/cb131e0f1390675e9f3219554f22bec1e387f75f.png)是上界，![](http://scikit-learn.org/stable/_images/math/7b1816c51f7d31275cd3ad400208fb7b3ce136a0.png)是![](http://scikit-learn.org/stable/_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png) by ![](http://scikit-learn.org/stable/_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png)的正半定矩阵，![](http://scikit-learn.org/stable/_images/math/778bba042f0a485da8e3297541320a94246be194.png)，  ![](http://scikit-learn.org/stable/_images/math/e8add116d50419f711117e1ff314e230537fccc8.png)是核。这里通过函数![](http://scikit-learn.org/stable/_images/math/10e009bdb83f96c5f47c58b34d5d4b12ef268d5b.png) 训练向量被隐式映射到高维(也许是无限维）空间中。

决策函数是：
<center>![](http://scikit-learn.org/stable/_images/math/9e6226cf29fbc353fca744defafbe44fee8db62b.png)</center>

这个参数可以通过`dual_coef_ `（持有![](http://scikit-learn.org/stable/_images/math/45566259f5632df57a1a4e87893103d00a4b5608.png)）获得。`support_vectors_`（持有支持向量），并且`intercept_ `持有独立项![](http://scikit-learn.org/stable/_images/math/f574498915fa9e02eeb5141c24835d077eba3e75.png)。

### 1.4.8. 实现细节
在内部，我们使用libsvm和liblinear处理所有的计算。这些库是用C和Cython包裹的。

