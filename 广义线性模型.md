## 1.1. 广义线性模型
下面介绍的是用于回归的一揽子方法，这些方法都基于这样一个假设：目标变量可以表现为输入变量的某种线性组合。数学概念表示如下（![](http://scikit-learn.org/stable/_images/math/4edbd88750539c2610a7bbfcf79c33cf1ae7a36c.png)表示预测值）：

<center>![](http://scikit-learn.org/stable/_images/math/dfdf17e3ecd9ca5506b2fbf5a7ebd70412326e81.png)
</center>

在上述模型中，我们指定![](http://scikit-learn.org/stable/_images/math/eb9ab421187ecd58130545c735c34483dc037fe1.png)为`coef_`，![](http://scikit-learn.org/stable/_images/math/87a0c2ec97d8b8f22868ec1242d2417f25d62240.png)为`intercept_`。

用广义线性模型进行分类，参见`Logistic regression`。<br>
### 1.1.1. 经典二乘法
**`LinearRegression`**拟合出一个系数![](http://scikit-learn.org/stable/_images/math/b0c860d08d30011cba6f6a97b98b32b8d747e51a.png)使得该模型预测的响应变量与数据中响应观测值得残差平方和最小。数学表示如下：

<div>![](http://scikit-learn.org/stable/_images/math/32028e85feb455d07503a027ba607eafc7909976.png)
</div>

<center><img src="http://scikit-learn.org/stable/_images/plot_ols_0011.png" style="width: 384.0px; height: 288.0px;"></center>


**`LinearRegression`**的`fit`方法接受数组形式的`X,y`，并将拟合出的线性模型系数![](http://scikit-learn.org/stable/_images/math/8659700e6646cd91bc02c32affaa5ec046ee9935.png)存储在它的`coef_`成员中：

<pre><code>>>> from sklearn import linear_model
>>> clf = linear_model.LinearRegression()
>>> clf.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> clf.coef_
array([ 0.5,  0.5])
</code></pre>

不过，经典二乘法估计的系数依赖于模型项的独立。当项是相关也就是说设计矩阵`X`的列线性依赖，设计矩阵接近*奇异*时，经典二乘法的预测将会对响应变量的随机误差高度敏感，从而产生一个很大的方差。这种*多重共线性*可能出现，例如，收集的数据没有经过实验设计。

#### 1.1.1.1. 经典二乘法的复杂度
该方法用`X`的奇异值分解求得最小二乘解。如果`X`是一个(n,p)的矩阵（假设![](http://scikit-learn.org/stable/_images/math/333132d227ff65accd034f936b7d5a43ea7493ee.png)），那么这个方法复杂度就是![](http://scikit-learn.org/stable/_images/math/012e7356d88340bea4c57c83c8e34a0bff43c99b.png)

### 1.1.2. Ridge回归
**`Ridge`**回归通过对系数大小进行惩罚解决了一些经典二乘法的问题。**`Ridge`**系数最小化一个添加了惩罚的残差平方和:

<center>![](http://scikit-learn.org/stable/_images/math/11f0787a645f4b5f2b810c0d00618785b58ff574.png)
</center>

这里，![](http://scikit-learn.org/stable/_images/math/c8b8590ededc6cdd4c311a1c5584090e60b95be4.png) 是一个复杂性参数，它控制了收缩量：![](http://scikit-learn.org/stable/_images/math/ad59b6e24a4a00ac621801f8d7513d68be654ab5.png)值越大，收缩量就越大并且系数的共线性就会变得更加的稳健。

<center><img src="http://scikit-learn.org/stable/_images/plot_ridge_path_0011.png" style="width: 384.0px; height: 288.0px;">
</center>

和其他线性模型一样，**`Ridge`**的`fit`方法接受数组形式的`X,y`，并将拟合出的线性模型系数![](http://scikit-learn.org/stable/_images/math/8659700e6646cd91bc02c32affaa5ec046ee9935.png)存储在它的`coef_`成员中：

<pre><code>>>> from sklearn import linear_model
>>> clf = linear_model.Ridge (alpha = .5)
>>> clf.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) 
Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)
>>> clf.coef_
array([ 0.34545455,  0.34545455])
>>> clf.intercept_ 
0.13636...
</pre></code>

#### 1.1.2.1. Ridge的时间复杂度
该方法与经典二乘法拥有相同的时间复杂度。

#### 1.1.2.2. 设置正则化参数：广义交叉验证
**`RidgeCV`**实现了内建关于alpha参数的交叉验证**`Ridge`**回归。该对象的工作方式除了默认的广义交叉验证`GCV`,与`GridSearchCV `也一样（一种留一验证的功效形式）：
<pre><code>>>> from sklearn import linear_model
>>> clf = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
>>> clf.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       
RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
    normalize=False)
>>> clf.alpha_                                      
0.1</code></pre>

### 1.1.3. Lasso
`Lasso`是一个估计稀疏系数的线性模型。它在一些情况下是有用的，原因是它倾向于提供较少的参数解，它能够有效地减少参数（给定的参数是依赖的）的个数。因为这个原因，`Lasso`和它的变种是传感压缩领域的根本。在一些确定的条件下，它可以恢复非零权重的精确集合。

它的数学表示是一个用L1范数作为正则项进行训练的线性模型。用于最小化的目标函数是：

<center>![](http://scikit-learn.org/stable/_images/math/5ff15825a85204658e3e5aa6e3b5952b8f709c27.png)
</center>

`Lasso`估计因此求解添加了![](http://scikit-learn.org/stable/_images/math/984dfa7241b6cabdc9e84f69458e973887308820.png)作为惩罚项的最小二乘法，这里![](http://scikit-learn.org/stable/_images/math/ad59b6e24a4a00ac621801f8d7513d68be654ab5.png)是一个常数，![](http://scikit-learn.org/stable/_images/math/a31e290885e74a51fa7f4d3e382e257a552587c8.png)是参数向量的L1范数。

`Lasso`类的实现利用坐标梯度下降来拟合系数。`Least Angle Regression`是另一种实现:
<pre><code>>>> from sklearn import linear_model
>>> clf = linear_model.Lasso(alpha = 0.1)
>>> clf.fit([[0, 0], [1, 1]], [0, 1])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection='cyclic', tol=0.0001, warm_start=False)
>>> clf.predict([[1, 1]])
array([ 0.8])
</code></pre>

函数`lasso_path`在所有可能的路径上计算系数，这对一些低级任务是十分有用的。

	Note： 利用Lasso进行特征选择

	由于Lasso回归产生稀疏模型，因此它可以用来做特征选择，具体细节参见 L1-based feature selection
<span>
	
	Note：随机稀疏
	
	对于特征选择或系数恢复，使用Randomized sparse models也许是有趣的。
<br>
#### 1.1.3.1. 设置正则化参数
`alpha`参数控制着估计系数的稀疏度。
##### 1.1.3.1.1. 使用交叉验证
`scikit-learn` 摆出的利用交叉验证设置`Lasso``alpha`参数的有：`LassoCV`和`LassoLarsCV`。`LassoLarsCV`是基于下面解释的` Least Angle Regression`算法。

对于高维的拥有很多共线回归量数据集，`LassoCV`是最常用最好的。然而，`LassoLarsCV`有探索更多与`alpha`参数相关值的优势，并且如果样本的数量相对于观测次数非常小的时候，它要比`LassoCV`更快。

<center>
<img alt="lasso_cv_2" src="http://scikit-learn.org/stable/_images/plot_lasso_model_selection_0021.png" style="width: 384.0px; height: 288.0px;">
<img alt="lasso_cv_2" src="http://scikit-learn.org/stable/_images/plot_lasso_model_selection_0031.png" style="width: 384.0px; height: 288.0px;">
</center>

#### 1.1.3.1.2. 基于信息准则的模型选择
除此之外，**`LassoLarsIC`**建议使用`AIC`和`BIC`来进行模型选择。它是发现`alpha`参数最优解的方式的一种计算廉价的替代品，因为当使用k-折交叉验证时，正则化路径仅被计算k+1次。然而，这种标准需要对解得自由度有一个正确的估计，对大样本的推导（渐进结果）和假定模型是正确的，例如：数据实际上就是由该模型产生的。它们也会被打破，当该问题有一个坏条件时（特征数大于样本数）。
<center>
<img alt="../_images/plot_lasso_model_selection_0011.png" src="http://scikit-learn.org/stable/_images/plot_lasso_model_selection_0011.png" style="width: 400.0px; height: 300.0px;">
</center>

### 1.1.4. 弹性网
**`ElasticNet`**是一种结合L1和L2正则化进行训练的线性回归模型。这种结合允许它在保持`Ridge`回归正则化性质的同时，像`Lasso`回归一样学习一个稀疏的模型（仅有少量的非零权重）。我们通过`l1_ratio`参数控制L1和L2的结合。

弹性网当多个特征相互相关时是有用的。`Lasso`可能会从中随机选取一个，而弹性网会全部选取。

在`Lasso`和`Ridge`中进行交易的一个实际优势是，它允许弹性网在下旋中继承一些`Ridge`的稳定性。

它最小化的目标函数如下：
<center>
![](http://scikit-learn.org/stable/_images/math/1ad2316c6e8615331c76273a683a0560d1e66d07.png)
</center>

<center>
<img alt="../_images/plot_lasso_model_selection_0011.png" src="http://scikit-learn.org/stable/_images/plot_lasso_coordinate_descent_path_0011.png" style="width: 400.0px; height: 300.0px;">
</center>

**`ElasticNetCV`**通过交叉验证来设置`alpha`和`l1_ratio`　<img class="math" src="http://scikit-learn.org/stable/_images/math/f574498915fa9e02eeb5141c24835d077eba3e75.png" alt="\rho">参数。

### 1.1.5. Multi-task Lasso
### 1.1.6. 最小角回归
最小角回归（**LARS**）是针对高维数据的一个回归算法，由Bradley Efron, Trevor Hastie, Iain Johnstone 和 Robert Tibshirani开发。

**LARS**的优点有：<br>
　　1. 当 p >> n时，它是算术高效的<br>
　　2. 它和后向选择计算速度一样并且与经典二乘法拥有相同时间复杂度。<br>
　　3. 它产生一个全分段的线性解路径，这对交叉验证和其他类似调整模型的尝试是有用的。<br>
　　4. 如果两个变量与响应变量几乎同等相关，那么他们的系数应该以大约同等速率增加。因此该算法的行为与直觉期望的一样，并且也更加的稳定。<br>
　　5. 它跟`Lasso`一样很容易都很容易进行修改来产生其他估计的解。

<br>
**LARS**的缺点有：<br>
　　因为LARS是基于对残差的迭代调整，它似乎对噪声的影响特别敏感。

**LARS**模型可以使用`Lars`，或者他的低级实现`lars_path`。

### 1.1.7. LARS Lasso
`LassoLars`是使用LARS算法实现的一个Lasso模型，并且不像基于坐标梯度下降的实现，它产生分段线性作为其系数范数函数的精确解。

<center>
<img alt="../_images/plot_lasso_model_selection_0011.png" src="http://scikit-learn.org/stable/_images/plot_lasso_lars_0011.png" style="width: 400.0px; height: 300.0px;">
</center>

<pre><code>>>> from sklearn import linear_model
>>> clf = linear_model.LassoLars(alpha=.1)
>>> clf.fit([[0, 0], [1, 1]], [0, 1])  
LassoLars(alpha=0.1, copy_X=True, eps=..., fit_intercept=True,
     fit_path=True, max_iter=500, normalize=True, positive=False,
     precompute='auto', verbose=False)
>>> clf.coef_    
array([ 0.717157...,  0.        ])
</code></pre>

该Lars算法几乎免费的提供沿着正则化参数的完整系数路径，因此使用`lars_path`函数是一个常见来检索路径的操作。

#### 1.1.7.1. 数学公式
该算法与前向逐步回归相似，但是与一步一步得到包含的变量不同，它通过在残差的每一个相关性角平分线方向上直接增加到估计的参数。

与给一个向量结果不同，LARS解包含一个表示参数向量的每一个L1范数值的曲线。完整的系数路径存储在`coef_path_`数组中，形状是 `(n_features, max_features+1)`。第一列总是零。

### 1.1.8. 正交匹配追踪（OMP）
### 1.1.9. 贝叶斯回归
### 1.1.10. Logistic regression
`Logistic regression`与它的名字不同，它是一个用于分类而非回归的线性模型。 `Logistic regression`在文献中也被称为`logit regression`，最大熵分类器，或者`log-linear` 分类器。在这个模型中，描述一次实验产生的可能结果的概率被用`logistic`函数模型化。

`scikit-learn`中`logistic regression`的实现可以通过`LogisticRegression`类获取。这个实现可以拟合一个多类（one-vs-rest）带有L2或L1进行正则化的`logistic regression`。

作为一个最优化问题，二类带有L2惩罚的 logistic regression最小化下面的代价函数：<br>
<center>![](http://scikit-learn.org/stable/_images/math/96fe247fe9465d26af15706141dc22e598ac7826.png)</center>

相似的，L1正则化的logistic regression求解下面的最优化问题：<br>
<center><img src="http://scikit-learn.org/stable/_images/math/3fb9bab302e67df4a9f00b8df259d326e01837fd.png" alt="\underset{w, c}{min\,} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) ."></center>

在类`LogisticRegression`中的求解实现有：“`liblinear`”,“`newton-cg`”,“`lbfgs`” and “`sag`”。

 “`lbfgs`” 和 “`newton-cg`”求解仅支持L2惩罚，但是对于高维数据它们收敛的速度要更快。L1惩罚产生稀疏解。

“`liblinear`”求解使用基于`Liblinear`库的坐标下降算法。对于L1惩罚`sklearn.svm.l1_min_c`允许计算更加小的C边界从而获得一个`null model`（所有的特征权重全为零）。这得益于和`scikit-learn`一起发送的优秀的`LIBLINEAR`库。然而由`LIBLINEAR`库实现的坐标下降算法并不能学习一个真正的多类模型，作为替代，最优化问题将多类分解为`one-vs-rest`作为一个二类分类器来学习。因此，`LogisticRegression`实例使用`liblinear`求解也可以表现为多类分类器。

在`LogisticRegression`中使用`lbfgs`或者`newton-cg`求解并设置`multi_class`为`multinomial`将会学习一个真正的多类`logistic regression`模型，这意味着它的可能性估计要比默认的`one-vs-rest`更好更准确。“`lbfgs`”，“`newton-cg`” 和 “`sag`”求解不能最优化L1惩罚模型，因此，`multinomial`设置无法学习出稀疏解。

`sag`求解使用随机平均梯度下降。他无法解决多类问题，并且被限制在L2惩罚模型，但是它对大规模数据（当样本数和特征数都非常大时）通常要比其他的求解器更快。

简而言之，可以通过下列规则来选择求解器：<br>
<table border="1" class="docutils">
<colgroup>
<col width="55%">
<col width="45%">
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Case</th>
<th class="head">Solver</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Small dataset or L1 penalty</td>
<td>“liblinear”</td>
</tr>
<tr class="row-odd"><td>Multinomial loss</td>
<td>“lbfgs” or newton-cg”</td>
</tr>
<tr class="row-even"><td>Large dataset</td>
<td>“sag”</td>
</tr>
</tbody>
</table>

对于大数据集，你也许会考虑使用带有‘`log`’损失的`SGDClassifier`。

    来自liblinear库的差异：
	
	当fit_intercept=False并且拟合出的coef_预测数据为0的时候，在使用solver=liblinear的LogisticRegression或LinearSVC和外面直接
	的liblinear库得出的分数之间会有差异。这是因为关于样本的decision_function为0的时，LogisticRegression and LinearSVC会预测负
	类，而liblinear会预测正类。记住，一个fit_intercept=False并且有很多样本的decision_function为0的模型，可能会欠拟合，这是一个
	不好的模型，你应该设置fit_intercept=True来提高截距的规模。

<span>

	Note: 稀疏logistic regression与特征选择
	
	一个带有L1惩罚的logistic regression会产生一个稀疏的模型，因此可以被用来进行特征选择，具体的细节参见L1-based feature selection。

内建交叉验证的`LogisticRegressionCV`可以发现最优的`C`参数。因为有预热启动， “`newton-cg`”, “`sag`” 和 “`lbfgs`”求解对于高维稠密数据来说是比较快的。如果`multi_class`设置为`ovr`，每一类都会包含一个最优的C参数解，如果`multi_class`设置为`multinomial`，将只包含一个最小化交叉熵损失的最优C值。

### 1.1.11. 随机梯度下降-SGD
随机梯度下降是一个简单但十分高效的拟合线性模型的方法。当样本数量（和特征数量）巨大的时候尤其有用。`partial_fit`方法允许`only/out-of-core`学习。

`SGDClassifier` 和 `SGDRegressor`为分类和回归提供了使用不同的损失函数和不同的惩罚项进行拟合线性模型的方法，例如`loss="log"`，`SGDClassifier`将拟合`logistic regression`模型，而`loss="hinge"`则拟合线性SVM。

### 1.1.12. 感知器
感知器`Perceptron`是大规模学习的简单算法。默认的：<br>
　　1. 它不需要学习率<br>
　　2. 它不需要正则项（惩罚项）<br>
　　3. 它只针对错误样本更新模型<br>

最后一个特征暗示了感知器要比训练一个`hinge `损失的SGD模型快一点，并且产生的模型是稀疏的。

### 1.1.13. 被动攻击学习
被动攻击学习算法是大规模学习家族的一员。它们和感知器一样都不需要学习率。然而，与感知器相反，它包含正则化参数`C`。

对于分类，`PassiveAggressiveClassifier`可以用`loss='hinge'`或`loss='squared_hinge'`。对于回归，`PassiveAggressiveRegressor`可以用`loss='epsilon_insensitive'`或`loss='squared_epsilon_insensitive'`。

### 1.1.14. 稳健回归：离群点和模型误差
稳健回归致力于在有损坏数据时拟合一个回归模型：无论是离群点，还是模型误差。

<center><img alt="../_images/plot_theilsen_0011.png" src="http://scikit-learn.org/stable/_images/plot_theilsen_0011.png" style="width: 400.0px; height: 300.0px;"></center>

#### 1.1.14.1 不同情景和有用的概念
思考下面的几种因为离群点数据而损坏的情形，

- Outliers in X or in y?

<table border="1" class="docutils">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Outliers in the y direction</th>
<th class="head">Outliers in the X direction</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="y_outliers" src="http://scikit-learn.org/stable/_images/plot_robust_fit_0031.png" style="width: 300.0px; height: 240.0px;"></a></td>
<td><a class="reference external" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="X_outliers" src="http://scikit-learn.org/stable/_images/plot_robust_fit_0021.png" style="width: 300.0px; height: 240.0px;"></a></td>
</tr>
</tbody>
</table>

- 离群与误差幅度的分数

离群点的数量是重要的，它们有多么异常也是重要的。
<table border="1" class="docutils">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Small outliers</th>
<th class="head">Large outliers</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="y_outliers" src="http://scikit-learn.org/stable/_images/plot_robust_fit_0031.png" style="width: 300.0px; height: 240.0px;"></a></td>
<td><a class="reference external" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="large_y_outliers" src="http://scikit-learn.org/stable/_images/plot_robust_fit_0051.png" style="width: 300.0px; height: 240.0px;"></a></td>
</tr>
</tbody>
</table>

稳健拟合的一个重要概念是击穿点：the fraction of data that can be outlying for the fit to start missing the inlying data.

简而言之，稳健拟合在高维设置时时很困难的。这里的稳健模型可能在这些设置下并不会工作。

	权衡：哪个预测器？
	
	Scikit-learn提供2种稳健回归预测器：RANSAC和Theil Sen
	- RANSAC更快，可随着样本数量好得多
	- RANSAC在y方向上的大离群点解决的更好
	- Theil Sen 可以更好的应付在X方向上中等大小的离群点，但是这一性质在高维设置时会消失。
	
	有疑惑时，就用RANSAC

#### 1.1.14.2. RANSAC:随机抽样一致性
RANSAC（随机抽样一致性）从完整数据集中随机抽出一组內围来拟合一个模型。

RANSAC是一个非确定性算法，有一个确定的概率去产生一个合理的结果，而这个概率依赖于迭代的次数（参见`max_trials `参数）。它通常用于线性和非线性的回归问题并且尤其在摄影计算机视觉领域流行。

这个算法将完整输入样本分裂成一组內围，这可能受噪声的影响，并且异常值是由于数据错误的测量和无效的验证引起的。这个结果模型是在仅仅有信心的內围数据上估计的。

<center><img alt="../_images/plot_ransac_0011.png" src="http://scikit-learn.org/stable/_images/plot_ransac_0011.png" style="width: 400.0px; height: 300.0px;"></center>

##### 1.1.14.2.1. 算法细节
每一次迭代执行下面几步：<br>
　　1.从原始数据中选择`min_samples `个随机样本并检查数据集合是否合理（`is_data_valid`）<br>
　　2.在随机子集上拟合一个模型（`base_estimator.fit`）并检查预测的模型是否合理（`is_model_valid`）<br>
　　3.通过计算预测模型的残差将所有的数据分类为內围点和离群点（`base_estimator.predict(X) - y`）所有数据样本的残差的绝对值小于`residual_threshold`被认为是內围点<br>
　　4.如果內围样本的数量达到最大值就将该模型存储为最佳模型。如果当前的模型有相同数量的內围点，那么取得分高的为最佳模型。

停止条件是要么满足特殊的停止条件（`stop_n_inliers`和`stop_score`）要么是达到最大次数（`max_trials`）。然后将会使用由先前的最佳模型决定的所有內围样本（共识集）来拟合最后的模型。

`is_data_valid` 和`is_model_valid`允许识别并拒绝随机子样本的退化组合。如果预测的模型不需要识别退化情况，`is_data_valid `应该在拟合模型前被使用以此来获得更好的计算性能。

#### 1.1.14.3. Theil-Sen 估计：基于广义中位数估计
`TheilSenRegressor`估计使用一种多维的广义中位数。因此它对多变量离群是稳健的。但是请注意估计的稳健性会随着问题的维度而急剧降低。在高维时，它会失去其稳健性并且不会比最小二乘法表现的好。

##### 1.1.14.3.1. 理论思考
`TheilSenRegressor`在渐进效率方面是最小二乘法的无偏估计。与OLS相反的是，Theil-Sen是一个非参数方法，这意味着它不对数据的底层分布做任何假设。因为Theil-Sen是一个基于中位数的估计，它针对损坏数据又名离群点具有更大稳健性。在单变量设置下，Theil-Sen在一个简单的线性回归下有大约29.3%的击穿点，这意味着它能容忍最多29.3%的数据损坏。

<center><img alt="../_images/plot_ransac_0011.png" src="http://scikit-learn.org/stable/_images/plot_theilsen_0011.png" style="width: 400.0px; height: 300.0px;"></center>

`scikit-learn`中的`TheilSenRegressor`的实现使用空间中位数（对多维中位数的一个推广）泛化到多元回归模型。

在时间和空间的复杂性方面，Theil-Sen依据<center>![](http://scikit-learn.org/stable/_images/math/35169538be4ee2d0427ecfbc80400da2647df66b.png)</center>

这使得他不可能完全的应用到样本数和特征数都很多的问题上。因此，通过在所有可能的结合中仅考虑一个随机子集以此来选择亚群的大小，以此可以限制时间和空间复杂度。

### 1.1.15. 多项式回归：扩展线性回归，基函数
在机器学习上一个常见的问题是如何在非线性函数的数据上使用线性模型进行训练。这种方法既保持了线性方法普遍快速的特性，同时又让他们能适应范围更广的数据。

例如，一个简单的线性回归可以通过构造多项式特征来进行扩展系数。在标准的线性回归例子中，你也许拥有像下面这样的二维数据模型：
<center>![](http://scikit-learn.org/stable/_images/math/5501d404995066141389603affa5de0656754bcb.png)</center>

如果你想拟合一个抛物面而不是平面的话，我们可以用二次多项式来结合特征，所以模型看上去像这样：
<center>![](http://scikit-learn.org/stable/_images/math/90f2579482275c7df4d0be9364a32b8b735cb99d.png)</center>

有时令人惊讶的观察是，这任然是一个线性模型：想象创造了一个新变量：
<center>![](http://scikit-learn.org/stable/_images/math/d9dd0082035232cea0abaf89472b5e9e28dcbdd5.png)</center>

对数据进行重新标签，我们的问题可以被重写成这样：
<center>![](http://scikit-learn.org/stable/_images/math/046a9803ee7eed69db4338f56ae9794e00961cb1.png)</center>

我们看这个多项式回归结果，和我们上面考虑的线性模型是同一类的，同样也可通过相同的技术来求解。通过考虑在一个由基函数建造的高维空间进行线性拟合，模型将会具有很大的灵活性，能够适应的数据范围更广。

这里有一个在一维数据上应用该思想的例子，
采用不同程度的多项式的特征：

<center><img alt="../_images/plot_ransac_0011.png" src="http://scikit-learn.org/stable/_images/plot_polynomial_interpolation_0011.png" style="width: 400.0px; height: 300.0px;"></center>

这个图使用` PolynomialFeatures`预处理器进行构造。这个预处理器将输入数据矩阵转换成一个给定程度的新的数据矩阵：
<pre><code>>>> from sklearn.preprocessing import PolynomialFeatures
>>> import numpy as np
>>> X = np.arange(6).reshape(3, 2)
>>> X
array([[0, 1],
       [2, 3],
       [4, 5]])
>>> poly = PolynomialFeatures(degree=2)
>>> poly.fit_transform(X)
array([[  1.,   0.,   1.,   0.,   0.,   1.],
       [  1.,   2.,   3.,   4.,   6.,   9.],
       [  1.,   4.,   5.,  16.,  20.,  25.]])
</code></pre>

X的特征从 ![](http://scikit-learn.org/stable/_images/math/96c49711bb53eae5b3697f1835b6eb50a9313fa8.png) 被转化成![](http://scikit-learn.org/stable/_images/math/ef3b1509f6875e9429212166fde17194086bdd4e.png),并且现在可以被应用到任何线性模型。

这个预处理步骤可以通过`Pipeline`工具进行流线化。代表一个简单的多项式回归对象可以用如下方式创建并使用：<br>
<pre><code>>>> from sklearn.preprocessing import PolynomialFeatures
>>> from sklearn.linear_model import LinearRegression
>>> from sklearn.pipeline import Pipeline
>>> import numpy as np
>>> model = Pipeline([('poly', PolynomialFeatures(degree=3)),
...                   ('linear', LinearRegression(fit_intercept=False))])
>>> # fit to an order-3 polynomial data
>>> x = np.arange(5)
>>> y = 3 - 2 * x + x ** 2 - x ** 3
>>> model = model.fit(x[:, np.newaxis], y)
>>> model.named_steps['linear'].coef_
array([ 3., -2.,  1., -1.])</code></pre>

这个训练好的多项式线性模型能够准确的恢复输入多项式系数。

在一些情况下，包含单一特征的高次方式没有必要的，但是只有在最多d个不同特征相乘叫做交互特征。这些可以通过设置`PolynomialFeatures`的`interaction_only=True`得到。

例如，当解决布尔特征，对于所有的n来说![](http://scikit-learn.org/stable/_images/math/3f282377e8ce2cfa70775ae54fbdb5981ec7b3cb.png),所以是不必要的；但是![](http://scikit-learn.org/stable/_images/math/dc8ce84d85fe65b5eecda998106e0cc716dbcc60.png)象征着两个布尔值的与运算。这样我们可以用一个线性分类器解决XOR问题：<br>
<pre><code>>>> from sklearn.linear_model import Perceptron
>>> from sklearn.preprocessing import PolynomialFeatures
>>> import numpy as np
>>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
>>> y = X[:, 0] ^ X[:, 1]
>>> X = PolynomialFeatures(interaction_only=True).fit_transform(X)
>>> X
array([[ 1.,  0.,  0.,  0.],
       [ 1.,  0.,  1.,  0.],
       [ 1.,  1.,  0.,  0.],
       [ 1.,  1.,  1.,  1.]])
>>> clf = Perceptron(fit_intercept=False, n_iter=10, shuffle=False).fit(X, y)
>>> clf.score(X, y)
1.0</code></pre>
